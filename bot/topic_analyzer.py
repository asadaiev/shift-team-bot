"""Topic analysis and summarization for messages."""
import re
from collections import Counter
from typing import List, Tuple, Dict, Optional
import logging

try:
    from sumy.parsers.plaintext import PlaintextParser
    from sumy.nlp.tokenizers import Tokenizer
    from sumy.summarizers.lsa import LsaSummarizer
    from sumy.summarizers.text_rank import TextRankSummarizer
    from sumy.nlp.stemmers import Stemmer
    from sumy.utils import get_stop_words
    SUMY_AVAILABLE = True
except ImportError:
    SUMY_AVAILABLE = False

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Track if OpenAI quota was exceeded to avoid repeated failed requests
_openai_quota_exceeded = False

from config import Config

logger = logging.getLogger(__name__)

# Common stop words (Ukrainian and English)
STOP_WORDS = {
    '—ñ', '—Ç–∞', '–∞–±–æ', '–∞–ª–µ', '—â–æ', '—è–∫', '–¥–ª—è', '–≤—ñ–¥', '–¥–æ', '–Ω–∞', '–∑', '–ø–æ', '–ø—Ä–æ', '–∑–∞', '–ø—Ä–∏',
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',
    '—Ü–µ', '—Ç–æ–π', '—Ç–∞–∫–∏–π', '—è–∫–∏–π', '–∫–æ–ª–∏', '–¥–µ', '—á–æ–º—É', '—è–∫—â–æ', '—Ö–æ—á–∞', '—Ç–æ–º—É', '—Ç–æ–º—É —â–æ',
    '—Ç–∞–∫', '–Ω—ñ', '–Ω–µ', '–Ω–µ', '–±—É–ª–æ', '–±—É–¥–µ', '—î', '–±—É–≤', '–±—É–ª–∞', '–±—É–ª–∏',
    '—è', '—Ç–∏', '–≤—ñ–Ω', '–≤–æ–Ω–∞', '–≤–æ–Ω–æ', '–º–∏', '–≤–∏', '–≤–æ–Ω–∏',
    '—â–æ–±', '—â–æ', '—è–∫–∏–π', '—è–∫–∞', '—è–∫–µ', '—è–∫—ñ',
    '—Ü–µ', '—Ü–µ–π', '—Ü—è', '—Ü–µ', '—Ü—ñ',
    '—Ç–æ–π', '—Ç–∞', '—Ç–µ', '—Ç—ñ',
    '–º', '—Ç', '–π', '–∂', '–±', '–∂', '—Ç–æ', '–∞', '–Ω—É', '–æ', '—É', '–µ', '–∏', '–æ', '–∞',
    # Add more stop words for better topic detection
    '—Ç–µ–±–µ', '–º–µ–Ω–µ', '–π–æ–≥–æ', '—ó—ó', '–Ω–∞—Å', '–≤–∞—Å', '—ó—Ö',
    '–ø—Ä–æ—Å—Ç–æ', '–¥—É–º–∞—é', '–Ω—ñ—Ö—É—è', '–Ω—ñ—á–æ–≥–æ', '–Ω—ñ–∫–æ–ª–∏', '–Ω—ñ–¥–µ', '–Ω—ñ–∫—É–¥–∏',
    '–º–æ–∂–µ', '–º–æ–∂–Ω–∞', '—Ç—Ä–µ–±–∞', '–ø–æ—Ç—Ä—ñ–±–Ω–æ', '–≤–∞—Ä—Ç–æ',
    '–±—É–ª–æ', '–±—É–¥–µ', '—î', '–±—É–≤', '–±—É–ª–∞', '–±—É–ª–∏',
    '—â–æ—Å—å', '—Ö—Ç–æ—Å—å', '–¥–µ—Å—å', '–∫—É–¥–∏—Å—å', '–∑–≤—ñ–¥–∫–∏—Å—å',
    '–≤–∂–µ', '—â–µ', '—Ç—ñ–ª—å–∫–∏', '–ª–∏—à–µ', '–Ω–∞–≤—ñ—Ç—å', '—Ç–∞–∫–æ–∂'
}

# Minimum word length to consider (increased to filter out short words)
MIN_WORD_LENGTH = 4

# Minimum frequency for a topic to be considered (lowered to include more topics)
MIN_TOPIC_FREQUENCY = 1


def extract_words(text: str) -> List[str]:
    """Extract meaningful words from text."""
    # Convert to lowercase and remove special characters
    text = text.lower()
    # Keep only letters, numbers, and Ukrainian characters
    text = re.sub(r'[^\w\s\u0400-\u04FF]', ' ', text)
    words = text.split()
    
    # Filter words
    filtered = []
    for word in words:
        # Remove very short words and stop words
        if len(word) >= MIN_WORD_LENGTH and word not in STOP_WORDS:
            filtered.append(word)
    
    return filtered


def analyze_topics(messages: List[Tuple[str, str, str]]) -> List[Tuple[str, int]]:
    """Analyze messages and extract main topics. Messages format: (user_id, username, message_text)."""
    if not messages:
        return []
    
    # Extract all words from messages
    all_words = []
    for user_id, username, message_text in messages:
        words = extract_words(message_text)
        all_words.extend(words)
    
    # Count word frequencies
    word_counts = Counter(all_words)
    
    # Get most common words (topics) - increased to 20 for more detail
    topics = word_counts.most_common(20)
    
    # Filter by minimum frequency
    filtered_topics = [(word, count) for word, count in topics if count >= MIN_TOPIC_FREQUENCY]
    
    return filtered_topics


def group_messages_by_topic(messages: List[Tuple[str, str, str]], topics: List[Tuple[str, int]]) -> Dict[str, List[Tuple[str, str]]]:
    """Group messages by detected topics. Returns Dict[topic, (first_mention, List[(username, full_message)])]."""
    topic_groups = {topic: [] for topic, _ in topics}
    topic_first_mention = {}  # Track who first mentioned each topic
    
    for user_id, username, message_text in messages:
        words = set(extract_words(message_text))
        # Find which topics are mentioned in this message
        for topic, _ in topics:
            if topic in words:
                # Track first mention
                if topic not in topic_first_mention:
                    topic_first_mention[topic] = username
                # Store full message (not just snippet) for detailed summary
                topic_groups[topic].append((username, message_text))
                break  # Only assign to first matching topic
    
    # Add first mention info to topic_groups
    for topic in topic_groups:
        if topic in topic_first_mention:
            topic_groups[topic] = (topic_first_mention[topic], topic_groups[topic])
    
    return topic_groups


def count_mentions(messages: List[Tuple[str, str, str]], username: str) -> int:
    """Count how many times a username was mentioned in messages."""
    if not messages:
        return 0
    
    count = 0
    username_lower = username.lower()
    for _, _, message_text in messages:
        # Count mentions (case-insensitive)
        text_lower = message_text.lower()
        # Count as word boundary to avoid partial matches
        count += len(re.findall(r'\b' + re.escape(username_lower) + r'\b', text_lower))
        # Also count @mentions
        count += text_lower.count('@' + username_lower)
    
    return count


def generate_topic_summary(messages: List[Tuple[str, str, str]]) -> str:
    """Generate a summary of topics discussed."""
    if not messages:
        return ""
    
    topics = analyze_topics(messages)
    if not topics:
        return ""
    
    lines = ["üí¨ <b>–û—Å–Ω–æ–≤–Ω—ñ —Ç–µ–º–∏ –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è:</b>", ""]
    
    for i, (topic, count) in enumerate(topics[:5], 1):  # Top 5 topics
        lines.append(f"{i}. <b>{topic}</b> ‚Äî –∑–≥–∞–¥—É–≤–∞–ª–æ—Å—å {count} —Ä–∞–∑—ñ–≤")
    
    return "\n".join(lines)


async def generate_text_summary(messages: List[Tuple[str, str, str]], language: str = "ukrainian") -> Optional[str]:
    """Generate detailed text summary by topics using sumy or OpenAI."""
    if not messages:
        logger.warning("generate_text_summary: no messages provided")
        return None
    
    # Combine all messages into one text
    all_text = "\n".join([msg for _, _, msg in messages])
    logger.info(f"generate_text_summary: combined text length: {len(all_text)}")
    
    if len(all_text) < 50:  # Too short to summarize
        logger.warning(f"generate_text_summary: text too short ({len(all_text)} chars), need at least 50")
        return None
    
    # Try OpenAI first if available and enabled (better quality)
    global _openai_quota_exceeded
    logger.info(f"generate_text_summary: checking OpenAI - USE_OPENAI_SUMMARY={Config.USE_OPENAI_SUMMARY}, OPENAI_AVAILABLE={OPENAI_AVAILABLE}, has_key={bool(Config.OPENAI_API_KEY)}, quota_exceeded={_openai_quota_exceeded}")
    if Config.USE_OPENAI_SUMMARY and OPENAI_AVAILABLE and Config.OPENAI_API_KEY and not _openai_quota_exceeded:
        try:
            logger.info("generate_text_summary: trying OpenAI")
            result = await generate_openai_detailed_summary(all_text, messages)
            if result:
                logger.info(f"generate_text_summary: OpenAI success, length: {len(result)}")
                return result
            else:
                logger.warning("generate_text_summary: OpenAI returned empty result")
        except Exception as e:
            logger.warning(f"OpenAI summarization failed: {e}, falling back to sumy", exc_info=True)
    else:
        logger.warning(f"generate_text_summary: OpenAI not available - USE_OPENAI_SUMMARY={Config.USE_OPENAI_SUMMARY}, OPENAI_AVAILABLE={OPENAI_AVAILABLE}, has_key={bool(Config.OPENAI_API_KEY)}")
    
    # Fallback to sumy (less detailed but works offline)
    if SUMY_AVAILABLE:
        try:
            logger.info("generate_text_summary: trying sumy")
            result = generate_sumy_detailed_summary(all_text, language)
            if result:
                logger.info(f"generate_text_summary: sumy success, length: {len(result)}")
                return result
            else:
                logger.warning("generate_text_summary: sumy returned empty result")
        except Exception as e:
            logger.warning(f"Sumy summarization failed: {e}", exc_info=True)
    else:
        logger.warning("generate_text_summary: SUMY_AVAILABLE is False, using simple fallback")
        # Simple fallback: generate topic-based summary without sumy
        return await generate_simple_fallback_summary(messages)
    
    logger.warning("generate_text_summary: all methods failed, using simple fallback")
    return await generate_simple_fallback_summary(messages)


def generate_sumy_summary(text: str, language: str = "ukrainian") -> str:
    """Generate summary using sumy library."""
    if not SUMY_AVAILABLE:
        return ""
    
    try:
        # Parse text
        parser = PlaintextParser.from_string(text, Tokenizer(language))
        
        # Use TextRank summarizer (works better for Ukrainian)
        summarizer = TextRankSummarizer(Stemmer(language))
        summarizer.stop_words = get_stop_words(language)
        
        # Generate summary (3-5 sentences)
        sentence_count = min(5, max(2, len(text.split('.')) // 10))
        summary_sentences = summarizer(parser.document, sentence_count)
        
        summary = " ".join([str(sentence) for sentence in summary_sentences])
        return summary.strip()
    except Exception as e:
        logger.error(f"Error generating sumy summary: {e}")
        return ""


async def generate_openai_detailed_summary(text: str, messages: List[Tuple[str, str, str]] = None) -> str:
    """Generate detailed summary by topics using OpenAI API."""
    global _openai_quota_exceeded
    if not OPENAI_AVAILABLE or not Config.OPENAI_API_KEY or _openai_quota_exceeded:
        return ""
    
    # If we have messages, use the same logic as generate_simple_fallback_summary to get topic names
    if messages:
        topics = analyze_topics(messages)
        if topics:
            topic_groups = group_messages_by_topic(messages, topics)
            # Use OpenAI to generate narratives for each topic
            result_lines = []
            topic_index = 0
            for topic, topic_data in list(topic_groups.items())[:8]:
                if isinstance(topic_data, tuple) and len(topic_data) == 2:
                    first_mention, topic_messages = topic_data
                    if topic_messages:
                        topic_index += 1
                        participants = set(username for username, _ in topic_messages)
                        participant_count = len(participants)
                        
                        # Get better topic name using OpenAI
                        better_topic_name = None
                        if not _openai_quota_exceeded:
                            try:
                                better_topic_name = await generate_topic_name(topic_messages)
                            except Exception as e:
                                logger.warning(f"Failed to generate topic name: {e}")
                        
                        display_topic = better_topic_name if better_topic_name else topic.capitalize()
                        
                        # Get all messages for this topic
                        all_topic_texts = [msg_text for _, msg_text in topic_messages]
                        topic_text = '\n'.join(all_topic_texts)
                        
                        # Generate narrative using OpenAI
                        narrative_text = None
                        if not _openai_quota_exceeded:
                            try:
                                narrative_text = await generate_topic_narrative(display_topic, topic_text, first_mention, participant_count)
                            except Exception as e:
                                logger.warning(f"Failed to generate OpenAI narrative: {e}")
                        
                        # Fallback: create simple narrative
                        if not narrative_text:
                            meaningful_messages = [msg.strip() for msg in all_topic_texts if len(msg.strip()) > 15]
                            if meaningful_messages:
                                narrative_text = ' '.join(meaningful_messages[:5])
                                narrative_text = ' '.join(narrative_text.split())
                                narrative_text = re.sub(r'[-‚Ä¢*]\s+', ' ', narrative_text)
                                narrative_text = re.sub(r'\d+\.\s+', ' ', narrative_text)
                                narrative_text = re.sub(r'[a-z]\)\s+', ' ', narrative_text)
                                if len(narrative_text) > 500:
                                    narrative_text = narrative_text[:500] + '...'
                        
                        if narrative_text:
                            # Ensure narrative starts with "–í —Ü—ñ–π —Ç–µ–º—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏..."
                            narrative_lower = narrative_text.lower().strip()
                            if not any(narrative_lower.startswith(prefix) for prefix in ('–≤ —Ü—ñ–π —Ç–µ–º—ñ', '–≤ —Ü—ñ–π', '–æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏', '–≥–æ–≤–æ—Ä–∏–ª–∏', '—Ä–æ–∑–º–æ–≤–ª—è–ª–∏')):
                                narrative_text = f"–í —Ü—ñ–π —Ç–µ–º—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏ {display_topic.lower()}. {narrative_text}"
                            
                            result_lines.append(f"{topic_index}. <b>{display_topic}</b> (–ø—ñ–¥–Ω—è–≤: <b>{first_mention}</b>, —É—á–∞—Å–Ω–∏–∫—ñ–≤: {participant_count})")
                            result_lines.append(f"   {narrative_text}")
                            
                            if len(topic_messages) > 5:
                                result_lines.append(f"   <i>... —Ç–∞ —â–µ {len(topic_messages) - 5} –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –ø—Ä–æ —Ü—é —Ç–µ–º—É</i>")
                            
                            # Add spacing between topics (except after the last one)
                            result_lines.append("")
                            result_lines.append("")
            
            if result_lines:
                # Remove trailing empty lines
                while result_lines and result_lines[-1] == "":
                    result_lines.pop()
                return "\n".join(result_lines)
    
    # Fallback to old method if no messages provided
    try:
        client = AsyncOpenAI(api_key=Config.OPENAI_API_KEY)
        
        # Truncate if too long (OpenAI has token limits)
        if len(text) > 12000:
            text = text[:12000] + "..."
        
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system", 
                    "content": "–¢–∏ –¥–æ–ø–æ–º–∞–≥–∞—î—à —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –∂–∏–≤—ñ —Ç–∞ –∑–º—ñ—Å—Ç–æ–≤–Ω—ñ summary –æ–±–≥–æ–≤–æ—Ä–µ–Ω—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é. –°—Ç–≤–æ—Ä—é–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –∑–≤—ñ—Ç –ø–æ —Ç–µ–º–∞—Ö, –¥–µ –∫–æ–∂–Ω–∞ —Ç–µ–º–∞ –æ–ø–∏—Å—É—î—Ç—å—Å—è –æ–¥–Ω–∏–º –∑–≤'—è–∑–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º –±–µ–∑ –ø—ñ–¥–ø—É–Ω–∫—Ç—ñ–≤ (–º–∞–∫—Å–∏–º—É–º 10 —Ä–µ—á–µ–Ω—å –Ω–∞ —Ç–µ–º—É, –∫—Ä–∞—â–µ 5-7). –ü–∏—à–∏ –∂–∏–≤–æ —Ç–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ: '–í —Ü—ñ–π —Ç–µ–º—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏ —Ç–∞–∫–µ-—Ç–æ —ñ —Ç–∞–∫–µ-—Ç–æ', –∑–≥–∞–¥—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –¥–µ—Ç–∞–ª—ñ —â–æ —Å–∞–º–µ –≥–æ–≤–æ—Ä–∏–ª–æ—Å—å, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∑–∞–≥–∞–ª—å–Ω–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥. –ö–æ–∂–Ω–∞ —Ç–µ–º–∞ –º–∞—î –±—É—Ç–∏ –æ–¥–Ω–∏–º –æ–ø–æ–≤—ñ–¥–∞—é—á–∏–º —Ç–µ–∫—Å—Ç–æ–º –∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–º–∏ –¥–µ—Ç–∞–ª—è–º–∏."
                },
                {
                    "role": "user", 
                    "content": f"–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –Ω–∞—Å—Ç—É–ø–Ω–µ –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ —Å—Ç–≤–æ—Ä–∏ summary —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é –≤ –ñ–ò–í–û–ú–£ –û–ü–û–í–Ü–î–ê–Æ–ß–û–ú–£ —Ñ–æ—Ä–º–∞—Ç—ñ. –û—Ä–≥–∞–Ω—ñ–∑—É–π –π–æ–≥–æ –ø–æ —Ç–µ–º–∞—Ö, –¥–µ –∫–æ–∂–Ω–∞ —Ç–µ–º–∞ - —Ü–µ –û–î–ò–ù –ó–í'–Ø–ó–ù–ò–ô –¢–ï–ö–°–¢ –±–µ–∑ –ø—ñ–¥–ø—É–Ω–∫—Ç—ñ–≤, —Å–ø–∏—Å–∫—ñ–≤, –º–∞—Ä–∫–µ—Ä—ñ–≤ (-, ‚Ä¢, 1., 2., a), b) —Ç–æ—â–æ). –ú–∞–∫—Å–∏–º—É–º 10 —Ä–µ—á–µ–Ω—å –Ω–∞ —Ç–µ–º—É (–∫—Ä–∞—â–µ 5-7). –ü–∏—à–∏ –∂–∏–≤–æ —Ç–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ: '–í —Ü—ñ–π —Ç–µ–º—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏ —Ç–∞–∫–µ-—Ç–æ —ñ —Ç–∞–∫–µ-—Ç–æ', –∑–≥–∞–¥—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –¥–µ—Ç–∞–ª—ñ —â–æ —Å–∞–º–µ –≥–æ–≤–æ—Ä–∏–ª–æ—Å—å, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∑–∞–≥–∞–ª—å–Ω–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥.\n\n–í–ê–ñ–õ–ò–í–û: –ü–∏—à–∏ –∂–∏–≤–æ —Ç–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ. –ù–∞–ø—Ä–∏–∫–ª–∞–¥:\n- '–í —Ü—ñ–π —Ç–µ–º—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏ –∑–º—ñ—ó, —è–∫—ñ —Ç—Ä–∞–ø–ª—è—é—Ç—å—Å—è –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –∫–æ–Ω—Ç–∏–Ω–µ–Ω—Ç–∞—Ö'\n- '–û–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏ –Ω–æ–≤—ñ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤ CS2, –∑–æ–∫—Ä–µ–º–∞ –∑–º—ñ–Ω–∏ –≤ –º–µ—Ö–∞–Ω—ñ—Ü—ñ —Å—Ç—Ä—ñ–ª—å–±–∏'\n- '–ì–æ–≤–æ—Ä–∏–ª–∏ –ø—Ä–æ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—É —Å–∏—Å—Ç–µ–º—É Faceit —Ç–∞ —è–∫ –ø—ñ–¥–≤–∏—â–∏—Ç–∏ —Å–≤—ñ–π Elo'\n\n–ó–ê–ë–û–†–û–ù–ï–ù–û –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:\n- –ú–∞—Ä–∫–µ—Ä–∏ (-, ‚Ä¢, *)\n- –ù—É–º–µ—Ä–æ–≤–∞–Ω—ñ —Å–ø–∏—Å–∫–∏ (1., 2., 3.)\n- –õ—ñ—Ç–µ—Ä–Ω—ñ —Å–ø–∏—Å–∫–∏ (a), b), c))\n- –ë—É–¥—å-—è–∫—ñ —ñ–Ω—à—ñ –ø—ñ–¥–ø—É–Ω–∫—Ç–∏\n\n–ü–†–ê–í–ò–õ–¨–ù–ò–ô —Ñ–æ—Ä–º–∞—Ç (–æ–¥–∏–Ω –∑–≤'—è–∑–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–∂–Ω–æ—ó —Ç–µ–º–∏):\nüéØ –¢–µ–º–∞ 1: –í —Ü—ñ–π —Ç–µ–º—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏ [–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ —Ç–µ–º–∞]. [–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –¥–µ—Ç–∞–ª—ñ —â–æ —Å–∞–º–µ –≥–æ–≤–æ—Ä–∏–ª–æ—Å—å]. [–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è]. [–í–∏—Å–Ω–æ–≤–∫–∏ –∞–±–æ —Ä—ñ—à–µ–Ω–Ω—è]. –í—Å–µ –æ–¥–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º –±–µ–∑ —Ä–æ–∑–±–∏—Ç—Ç—è –Ω–∞ –ø—É–Ω–∫—Ç–∏.\nüéØ –¢–µ–º–∞ 2: –í —Ü—ñ–π —Ç–µ–º—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏ [—ñ–Ω—à–∞ —Ç–µ–º–∞]. [–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –¥–µ—Ç–∞–ª—ñ]. –¢–∞–∫–æ–∂ –æ–¥–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º.\n\n–û–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è:\n\n{text}"
                }
            ],
            max_tokens=800,
            temperature=0.7
        )
        
        summary = response.choices[0].message.content.strip()
        # Remove any duplicate topic headers that might be in the text
        summary = re.sub(r'üéØ\s*[^(]*\([^)]*\)[^.]*\.?\s*', '', summary)
        summary = re.sub(r'üéØ\s*[^:]*:\s*', '', summary)
        
        # Remove bullet points and numbered lists within topics - more aggressive cleaning
        # Remove bullet points (-, ‚Ä¢, *, etc.)
        summary = re.sub(r'\n\s*[-‚Ä¢*]\s+', ' ', summary)
        summary = re.sub(r'[-‚Ä¢*]\s+', ' ', summary)
        # Remove numbered lists (1., 2., etc.)
        summary = re.sub(r'\n\s*\d+\.\s+', ' ', summary)
        summary = re.sub(r'\d+\.\s+', ' ', summary)
        # Remove any remaining list markers
        summary = re.sub(r'\n\s*[a-z]\)\s+', ' ', summary)  # a), b), c)
        summary = re.sub(r'[a-z]\)\s+', ' ', summary)
        
        # Split by topic headers and clean each topic
        topics = re.split(r'üéØ\s*[^:]+:', summary)
        cleaned_topics = []
        for topic in topics:
            if topic.strip():
                # Remove all list markers from topic text
                topic = re.sub(r'[-‚Ä¢*]\s+', ' ', topic)
                topic = re.sub(r'\d+\.\s+', ' ', topic)
                topic = re.sub(r'[a-z]\)\s+', ' ', topic)
                # Replace multiple newlines with single space
                topic = re.sub(r'\n+', ' ', topic)
                # Clean up multiple spaces
                topic = ' '.join(topic.split())
                if topic.strip():
                    cleaned_topics.append(topic.strip())
        
        # Reconstruct summary with cleaned topics
        if cleaned_topics:
            result_lines = []
            topic_headers = re.findall(r'üéØ\s*[^:]+:', summary)
            for i, header in enumerate(topic_headers):
                if i < len(cleaned_topics):
                    result_lines.append(f"{header.strip()} {cleaned_topics[i]}")
            summary = '\n\n'.join(result_lines) if result_lines else summary
        else:
            # Fallback: just clean the original text
            summary = re.sub(r'\n+', ' ', summary)
            summary = ' '.join(summary.split())
        
        return summary.strip()
    except Exception as e:
        error_str = str(e).lower()
        # Check for quota exceeded (429) or insufficient quota
        if '429' in error_str or 'insufficient_quota' in error_str or 'quota' in error_str:
            logger.error(f"OpenAI quota exceeded, disabling OpenAI for this session: {e}")
            # Mark quota as exceeded to avoid repeated failed requests
            _openai_quota_exceeded = True
        else:
            logger.error(f"Error generating OpenAI summary: {e}")
        return ""


def generate_sumy_detailed_summary(text: str, language: str = "ukrainian") -> str:
    """Generate detailed summary using sumy library (grouped by topics)."""
    if not SUMY_AVAILABLE:
        return ""
    
    try:
        # Parse text
        parser = PlaintextParser.from_string(text, Tokenizer(language))
        
        # Use TextRank summarizer
        summarizer = TextRankSummarizer(Stemmer(language))
        summarizer.stop_words = get_stop_words(language)
        
        # Generate more sentences for detailed summary (increase from 8 to 12-15)
        sentence_count = min(15, max(8, len(text.split('.')) // 5))
        summary_sentences = summarizer(parser.document, sentence_count)
        
        # Group sentences by topics (simple approach: by keywords)
        topics = analyze_topics([("", text)])
        if topics:
            # Create detailed topic-based summary
            lines = []
            for topic, _ in topics[:8]:  # Top 8 topics for more detail
                # Find sentences mentioning this topic
                topic_sentences = [
                    str(s) for s in summary_sentences 
                    if topic.lower() in str(s).lower()
                ]
                if topic_sentences:
                    # Show more sentences per topic (3-4 instead of 2)
                    sentences_text = ' '.join(topic_sentences[:4])
                    lines.append(f"üéØ <b>{topic.capitalize()}</b>: {sentences_text}")
            
            if lines:
                return "\n".join(lines)
        
        # Fallback: detailed summary with all sentences
        summary = " ".join([str(sentence) for sentence in summary_sentences])
        return summary.strip()
    except Exception as e:
        logger.error(f"Error generating sumy summary: {e}")
        return ""


async def generate_topic_name(topic_messages: List[Tuple[str, str]]) -> Optional[str]:
    """Generate a better topic name using OpenAI based on message content."""
    global _openai_quota_exceeded
    if not Config.USE_OPENAI_SUMMARY or not OPENAI_AVAILABLE or not Config.OPENAI_API_KEY or _openai_quota_exceeded:
        return None
    
    if not topic_messages:
        return None
    
    try:
        client = AsyncOpenAI(api_key=Config.OPENAI_API_KEY)
        
        # Get all messages for this topic
        all_texts = [msg_text for _, msg_text in topic_messages[:10]]  # Use first 10 messages
        topic_text = '\n'.join(all_texts)
        
        # Truncate if too long
        if len(topic_text) > 1500:
            topic_text = topic_text[:1500] + "..."
        
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": "–¢–∏ –¥–æ–ø–æ–º–∞–≥–∞—î—à –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∫–æ—Ä–æ—Ç–∫—ñ, —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ñ –Ω–∞–∑–≤–∏ —Ç–µ–º –æ–±–≥–æ–≤–æ—Ä–µ–Ω—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é. –ù–∞–∑–≤–∞ –º–∞—î –±—É—Ç–∏ 1-3 —Å–ª–æ–≤–∞, —â–æ —Ç–æ—á–Ω–æ –æ–ø–∏—Å—É—é—Ç—å –ø—Ä–æ —â–æ –π–¥–µ—Ç—å—Å—è –≤ –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—ñ."
                },
                {
                    "role": "user",
                    "content": f"–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –Ω–∞—Å—Ç—É–ø–Ω—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —Ç–∞ —Å—Ç–≤–æ—Ä–∏ –∫–æ—Ä–æ—Ç–∫—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—É –Ω–∞–∑–≤—É —Ç–µ–º–∏ (1-3 —Å–ª–æ–≤–∞) —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é, —â–æ —Ç–æ—á–Ω–æ –æ–ø–∏—Å—É—î –ø—Ä–æ —â–æ –π–¥–µ—Ç—å—Å—è. –ù–∞–∑–≤–∞ –º–∞—î –±—É—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—é —Ç–∞ –∑–º—ñ—Å—Ç–æ–≤–Ω–æ—é, –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä—à–µ —Å–ª–æ–≤–æ –∑ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è.\n\n–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è:\n{topic_text}\n\n–ù–∞–∑–≤–∞ —Ç–µ–º–∏ (—Ç—ñ–ª—å–∫–∏ –Ω–∞–∑–≤–∞, –±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç—É):"
                }
            ],
            max_tokens=20,
            temperature=0.3
        )
        
        topic_name = response.choices[0].message.content.strip()
        # Remove quotes if present
        topic_name = topic_name.strip('"\'')
        return topic_name if topic_name else None
    except Exception as e:
        error_str = str(e).lower()
        # Check for quota exceeded (429) or insufficient quota
        if '429' in error_str or 'insufficient_quota' in error_str or 'quota' in error_str:
            logger.warning(f"OpenAI quota exceeded, skipping topic name generation: {e}")
            _openai_quota_exceeded = True
        else:
            logger.warning(f"Error generating topic name: {e}")
        return None


async def generate_simple_fallback_summary(messages: List[Tuple[str, str, str]]) -> str:
    """Generate a detailed topic-based summary. Uses OpenAI for better narrative if available."""
    if not messages:
        return ""
    
    # Analyze topics
    topics = analyze_topics(messages)
    if not topics:
        # If no topics found, provide detailed summary of all messages
        lines = []
        lines.append("üìã <b>–î–µ—Ç–∞–ª—å–Ω–∏–π –∑–º—ñ—Å—Ç –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è:</b>")
        lines.append("")
        for i, (_, username, msg) in enumerate(messages[:10], 1):
            # Show longer snippets (up to 300 chars)
            snippet = msg[:300] + ('...' if len(msg) > 300 else '')
            lines.append(f"{i}. <b>{username}</b>: {snippet}")
        return "\n".join(lines) if lines else ""
    
    # Group messages by topics
    topic_groups = group_messages_by_topic(messages, topics)
    
    # Generate detailed summary by topics in narrative format
    lines = []
    topic_index = 0
    for topic, topic_data in list(topic_groups.items())[:8]:  # Top 8 topics for more detail
        topic_index += 1
        if isinstance(topic_data, tuple) and len(topic_data) == 2:
            first_mention, topic_messages = topic_data
            if topic_messages:
                # Count unique participants
                participants = set(username for username, _ in topic_messages)
                participant_count = len(participants)
                
                # Try to generate better topic name using OpenAI
                better_topic_name = None
                global _openai_quota_exceeded
                if Config.USE_OPENAI_SUMMARY and OPENAI_AVAILABLE and Config.OPENAI_API_KEY and not _openai_quota_exceeded:
                    try:
                        better_topic_name = await generate_topic_name(topic_messages)
                    except Exception as e:
                        logger.warning(f"Failed to generate topic name: {e}")
                
                # Use better name if available, otherwise use original
                display_topic = better_topic_name if better_topic_name else topic.capitalize()
                
                # Get all messages for this topic
                all_topic_texts = [msg_text for _, msg_text in topic_messages]
                topic_text = '\n'.join(all_topic_texts)
                
                # Try to use OpenAI for better narrative if available
                narrative_text = None
                if Config.USE_OPENAI_SUMMARY and OPENAI_AVAILABLE and Config.OPENAI_API_KEY and not _openai_quota_exceeded:
                    try:
                        narrative_text = await generate_topic_narrative(display_topic, topic_text, first_mention, participant_count)
                    except Exception as e:
                        logger.warning(f"Failed to generate OpenAI narrative for topic {display_topic}: {e}")
                
                # Fallback: create simple narrative from messages
                if not narrative_text:
                    meaningful_messages = [msg.strip() for msg in all_topic_texts if len(msg.strip()) > 15]
                    if meaningful_messages:
                        # Take first 3-5 meaningful messages
                        selected_messages = meaningful_messages[:5]
                        # Try to create a more coherent narrative
                        narrative_text = ' '.join(selected_messages)
                        # Clean up: remove excessive punctuation and spaces
                        narrative_text = ' '.join(narrative_text.split())
                        # Remove any topic headers that might be in the text (like "üéØ –¢–µ–º–∞:" or "üéØ –¢–µ–º–∞ (–ø—ñ–¥–Ω—è–≤: ...)")
                        narrative_text = re.sub(r'üéØ\s*[^(]*\([^)]*\)[^.]*\.?\s*', '', narrative_text)
                        narrative_text = re.sub(r'üéØ\s*[^:]*:\s*', '', narrative_text)
                        narrative_text = narrative_text.strip()
                        # Limit to 400-500 chars for readability
                        if len(narrative_text) > 500:
                            narrative_text = narrative_text[:500] + '...'
                
                if narrative_text:
                    # Ensure narrative starts with "–í —Ü—ñ–π —Ç–µ–º—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏..." if it doesn't already
                    narrative_lower = narrative_text.lower().strip()
                    if not any(narrative_lower.startswith(prefix) for prefix in ('–≤ —Ü—ñ–π —Ç–µ–º—ñ', '–≤ —Ü—ñ–π', '–æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏', '–≥–æ–≤–æ—Ä–∏–ª–∏', '—Ä–æ–∑–º–æ–≤–ª—è–ª–∏')):
                        narrative_text = f"–í —Ü—ñ–π —Ç–µ–º—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏ {display_topic.lower()}. {narrative_text}"
                    
                    lines.append(f"")
                    lines.append(f"{topic_index}. <b>{display_topic}</b> (–ø—ñ–¥–Ω—è–≤: <b>{first_mention}</b>, —É—á–∞—Å–Ω–∏–∫—ñ–≤: {participant_count})")
                    lines.append(f"   {narrative_text}")
                    
                    if len(topic_messages) > 5:
                        lines.append(f"   <i>... —Ç–∞ —â–µ {len(topic_messages) - 5} –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –ø—Ä–æ —Ü—é —Ç–µ–º—É</i>")
                    
                    # Add spacing between topics (except after the last one)
                    lines.append("")
                    lines.append("")
        elif topic_data:
            # Fallback for old format
            snippet_texts = [snippet if isinstance(snippet, str) else snippet[1] for snippet in topic_data[:4]]
            topic_summary = ' '.join(snippet_texts)
            if len(topic_summary) > 400:
                topic_summary = topic_summary[:400] + '...'
            lines.append(f"üéØ <b>{topic.capitalize()}</b>: {topic_summary}")
    
    if lines:
        # Remove trailing empty lines
        while lines and lines[-1] == "":
            lines.pop()
        return "\n".join(lines)
    return ""


async def generate_topic_narrative(topic: str, topic_messages: str, first_mention: str, participant_count: int) -> Optional[str]:
    """Generate narrative summary for a specific topic using OpenAI."""
    global _openai_quota_exceeded
    if not OPENAI_AVAILABLE or not Config.OPENAI_API_KEY or _openai_quota_exceeded:
        return None
    
    try:
        client = AsyncOpenAI(api_key=Config.OPENAI_API_KEY)
        
        # Truncate if too long
        if len(topic_messages) > 2000:
            topic_messages = topic_messages[:2000] + "..."
        
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": "–¢–∏ –¥–æ–ø–æ–º–∞–≥–∞—î—à —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –æ–ø–æ–≤—ñ–¥–∞—é—á—ñ summary –æ–±–≥–æ–≤–æ—Ä–µ–Ω—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é. –°—Ç–≤–æ—Ä—é–π –∑–≤'—è–∑–Ω–∏–π, –æ–ø–æ–≤—ñ–¥–∞—é—á–∏–π —Ç–µ–∫—Å—Ç —â–æ –æ–ø–∏—Å—É—î —â–æ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–æ—Å—å, –±–µ–∑ –∑–≥–∞–¥–æ–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —ñ–º–µ–Ω –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ (—è–∫—â–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–æ)."
                },
                {
                    "role": "user",
                    "content": f"–°—Ç–≤–æ—Ä–∏ –∂–∏–≤–∏–π –æ–ø–æ–≤—ñ–¥–∞—é—á–∏–π summary (–º–∞–∫—Å–∏–º—É–º 10 —Ä–µ—á–µ–Ω—å, –∫—Ä–∞—â–µ 5-7) –ø—Ä–æ —Ç–µ, —â–æ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–æ—Å—å –Ω–∞ —Ç–µ–º—É '{topic}'. –ü–æ—á–Ω–∏ –∑ '–í —Ü—ñ–π —Ç–µ–º—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏ [–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ —Ç–µ–º–∞]' —ñ –¥–∞–ª—ñ –æ–ø–∏—à–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –¥–µ—Ç–∞–ª—ñ —â–æ —Å–∞–º–µ –≥–æ–≤–æ—Ä–∏–ª–æ—Å—å, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ –æ–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∑–∞–≥–∞–ª—å–Ω–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥. –¢–µ–∫—Å—Ç –º–∞—î –±—É—Ç–∏ –∑–≤'—è–∑–Ω–∏–º, –æ–ø–æ–≤—ñ–¥–∞—é—á–∏–º, –æ–¥–Ω–∏–º —Å—É—Ü—ñ–ª—å–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º –ë–ï–ó –ø—ñ–¥–ø—É–Ω–∫—Ç—ñ–≤, —Å–ø–∏—Å–∫—ñ–≤, –º–∞—Ä–∫–µ—Ä—ñ–≤ (-, ‚Ä¢, 1., 2. —Ç–æ—â–æ). –ù–µ –∑–≥–∞–¥—É–π —ñ–º–µ–Ω–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤, —è–∫—â–æ —Ü–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–æ. –ü–∏—à–∏ –∂–∏–≤–æ —Ç–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –∑ –¥–µ—Ç–∞–ª—è–º–∏ —â–æ —Å–∞–º–µ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–æ—Å—å.\n\n–í–ê–ñ–õ–ò–í–û: –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –º–∞—Ä–∫–µ—Ä–∏, —Å–ø–∏—Å–∫–∏ –∞–±–æ –ø—ñ–¥–ø—É–Ω–∫—Ç–∏. –¢—ñ–ª—å–∫–∏ –æ–¥–∏–Ω –∑–≤'—è–∑–Ω–∏–π —Ç–µ–∫—Å—Ç. –ü–æ—á–Ω–∏ –∑ '–í —Ü—ñ–π —Ç–µ–º—ñ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏...' —ñ –¥–∞–ª—ñ –æ–ø–∏—à–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –¥–µ—Ç–∞–ª—ñ.\n\n–û–±–≥–æ–≤–æ—Ä–µ–Ω–Ω—è:\n{topic_messages}"
                }
            ],
            max_tokens=300,
            temperature=0.7
        )
        
        narrative = response.choices[0].message.content.strip()
        # Remove any topic headers that might be in the text (like "üéØ –¢–µ–º–∞:" or "üéØ –¢–µ–º–∞ (–ø—ñ–¥–Ω—è–≤: ...)")
        narrative = re.sub(r'üéØ\s*[^(]*\([^)]*\)[^.]*\.?\s*', '', narrative)
        narrative = re.sub(r'üéØ\s*[^:]*:\s*', '', narrative)
        # Remove bullet points and numbered lists - aggressive cleaning
        narrative = re.sub(r'\n\s*[-‚Ä¢*]\s+', ' ', narrative)
        narrative = re.sub(r'[-‚Ä¢*]\s+', ' ', narrative)
        narrative = re.sub(r'\n\s*\d+\.\s+', ' ', narrative)
        narrative = re.sub(r'\d+\.\s+', ' ', narrative)
        narrative = re.sub(r'\n\s*[a-z]\)\s+', ' ', narrative)
        narrative = re.sub(r'[a-z]\)\s+', ' ', narrative)
        # Replace multiple newlines with single space
        narrative = re.sub(r'\n+', ' ', narrative)
        # Clean up multiple spaces
        narrative = ' '.join(narrative.split())
        return narrative.strip()
    except Exception as e:
        error_str = str(e).lower()
        # Check for quota exceeded (429) or insufficient quota
        if '429' in error_str or 'insufficient_quota' in error_str or 'quota' in error_str:
            logger.warning(f"OpenAI quota exceeded, skipping topic narrative: {e}")
            _openai_quota_exceeded = True
        else:
            logger.warning(f"Error generating topic narrative: {e}")
        return None
